
\chapter{Introduction}
\label{introduction}

Optimization is a process to obtain a minimal (or maximal) result for a defined problem. Problems can range from simple functions like $f(x) = x^2$ to multiple functions that make up a complex model (e.g. determining the water saturation in a fuel cell). By optimizing these types of problems, known as objective functions, parameters for a given objective function are solved to minimize the result. This can lead to new discoveries in research and further the development of new technologies.

Optimization can be local or global. Local optimization searches in a given neighbourhood around a provided candidate solution. The search obtains a local minimum value in the neighbourhood of the domain based on satisfying converging properties. When searching for global minimum in a case where the local minimum is not guaranteed to be a global minimum, a global solver is needed. Unlike the local solver, a global optimization will search the whole domain (rather than a particular neighbourhood). By doing so, the global optimization process determines a global minimum in a certain amount of time. There are two types of global optimization: deterministic and stochastic \cite{Liberti2000}. These types of global optimizations have found global solutions for  applications including chemical equilibrium, nuclear reactors, curve fitting, vehicle design and cost \cite{Pinter2002}, and many others.

Deterministic algorithms guarantee a global minimum, but take a large amount of time to do so insofar as they search the whole domain. In the worst cases, these algorithms have complexities of exponential time. In cases where large amounts of time are not practical, stochastic algorithms are used instead. 

Stochastic algorithms cannot guarantee a global minimum because of the adaptive random searches they deliver. However, they can determine a good candidate minimum in an amount of time specified by the user. Researchers tend to use these algorithms if the exact global minimum is not needed to solve the objective function. There are also various stochastic algorithms that may perform better on specific types of objective functions. Some examples of such algorithms are Particle Swarm Optimization \cite{Kennedy}, Genetic Algorithm \cite{}, and Simulated Annealing \cite{}. 

Various types of objective functions lead to various challenges for the global solver. One kind of challenge is premature local convergence, which happens when a global solver gets stuck exploring a local minimum. This wastes time that could be spent finding a global minimum; it can also lead to longer optimization times for deterministic algorithms, or to a poor global candidate for stochastic algorithms. Objective functions that have the potential for premature local convergence are non-convex functions for which local minima are not guaranteed to be the global minimum. Thus a solver gets stuck in a local minimum, making it difficult to find a global minimum value. 

Failure can also occur in the objective function itself. The optimization process might face a graceful shutdown, which requires restarting the process and re-evaluating the objective function in hopes a failure does not occur. One type of failure is the result of a resource contention. Resource contention can occur when both parallel global optimization algorithms and the objective function use up various resources (processors and memory), or when multiple objective functions that demand more resource power are running simultaneously. Depending on the machine and the algorithms policy, resource contention can lead to serial optimization or failure. Running serially can therefore defeat the purpose of speeding up the optimization, since the process will needed to be restarted. 

Another challenge for global optimization involves monitoring the objective function to determine whether further action is needed, or in order to obtain extra information. The user has the flexibility to change various properties of the function. This can affect the sensitivity of the optimization process, often increasing or decreasing the time required to obtain a global minimum. The user may also want to derive external information from the objective function, such as 
\begin{itemize}
  \item specific properties of the model, 
  \item post processing of data, or
  \item the top $N$ solutions.
\end{itemize}

My contributions to solving these challenges are the software Computefarm and the Optimization Database. Computefarm is a distributed system that utilizes unused computer resources on various client computers to run multiple function evaluations at once. It can speed up the iteration process of the given solver, and thereby speed up the search for the global minimum in a black-box problem. It is also fault tolerant to failures of a farmed computer by allowing the global optimization process to continue without restarting. 

The Optimization Database is a flexible database that allows the model or solver to store the result and any extra information pertaining to the simulation or post processing of the data. With this information the user can i) determine if the solver is stuck at a local minimum, ii) initiate other solvers based on currently stored data, or iii) use the data to further analyse the model. 

In this thesis we apply Computefarm and Optimization Database to two applications: quantum error correction circuit design and crystal structure prediction. In the first problem, a circuit is designed to correct for errors in quantum-processor systems. Unlike transistor-based computers, quantum computers cannot be controlled using a software-based design. Instead, they are controlled directly from a circuit component. This makes the process of manufacturing circuits and the testing of desired reliability quite costly. In light of this, several models have been designed to simulate a quantum error correction circuit for the purpose of determining the effect of error correction on a given $n$-qubit system. To ensure high reliability (also known as feasibility) of the circuit design, the circuit parameters are optimized so that the model returns a desired feasibility. In the present application the desired feasibility is $99.99\%$ for an encryption, decryption circuit \cite{Barends2014,Ghosh2013}. This is the highest modelled feasibility needed for external noise when manufactured. This feasibility value has been obtained for the $3$-qubit system \cite{Zahedinejad2014}. In this thesis, the $4$-qubit system is reached using global optimization algorithms and the software applications.   

Crystal structure prediction has been used in the past decade to approximate the most stable structure of a compound at a particular temperature and pressure environment. Because there is a large variety of lattice positions for a given compound, testing every possible lattice structure at a desired temperature and pressure can be taxing. Researchers thus use Ab Initio structure codes to calculate properties of the given lattice in the hope of discovering a new structure for a given compound. One particular property that Ab Initio codes return is the total energy of the provided lattice structure. This energy represents the stability of the structure in the given environment. The lower the energy, the more stable the structure and the potential of having interesting properties. For example, finding a structure harder than diamond seems impossible experimentally; however, by globally optimizing a specific compound in a given environment, a new stable structure can be discovered. Another region of interest for structure prediction is the development of crystals for electronics. Crystals have resonating frequencies that make them good materials for signal timing components in devices like wristwatches and computer-processor chips. Global optimization is needed to find the most stable structures for the crystal compound to further determine properties like the optimal resonant frequency. In this thesis, the diatomic structure of silicon dioxide is optimized to generate the most stable lattice structure of a quartz crystal, and to further understand the variations of each structure and their crystal properties. 

Chapter \ref{background} gives the background on global optimization algorithms and the properties that objective functions present for the optimization process. Chapter \ref{methods} documents the software developed, Computefarm and the Optimization Database. Chapter \ref{applications} describes the two applications, quantum error correction circuit design and crystal structure prediction of silicon dioxide, as well as how the software was used to aid in
solving these problems. The final Chapter \ref{conclusion} summarizes the results obtained for the two applications and the benefits the software had for solving the applications.

