\chapter{Global Optimization}
\label{background}
Global optimization is widely used method used in engineering, chemistry, economics and etc, for obtain the extreme minimum (or maximum) value. By formulating real world problems to obtain the global minimum in the form of
\begin{align}
  \label{eq:GO}
  x^* = \arg \min_{x \epsilon D} f(x)\\
  f:D\rightarrow \mathbb{R} \\ 
  D = {x,x\epsilon \mathbb{R}^{N},l_n\leq x_n \leq u_n, n = 1,\ldots,N}
\end{align}

where $f(x)$ is the objective function, x is a $N$-dimensional vector between the lower bound $l_n$ and upper bound $u_n$ for the $n$th variable in the domain $D$ and $x^*$ denotes the global minimum of the objective function in the given domain $D$. This form of global optimization \ref{eq:GO} represents an unconstrained global optimization expression, such that to add constraints the form would include 
\begin{align}
  \label{eq:constrainedGO}
    g(x) \leq 0, \\
    g(x) = 0
\end{align}

In the 1950's exhaustive searches like the Simplex algorithm \cite{Liberti2000} used this form \ref{eq:GO} to obtain the global minimum by searching over ever possible point in the domain. This method of searching did guarantee the global minimum over enough time to search every possible combination. This became non-practical to users as objective functions became more complex and domains larger. In the most recent years, research has looked into the characteristics of the
objective function, the constraints and algorithms to obtain more efficient methods that can or obtain close to the global minimum. 

\section{Objective Functions}
An objective function is the computation that the global optimization is solving. This function can contain a single function or multiple functions to represent a model of a real world problem. These functions can presents multiple characteristics that can determine the difficulty to optimize in the given domain, some examples are
\begin{itemize}
  \item continuity,
  \item smoothness,
  \item convexity,
  \item differentiable, and
  \item computational time.
\end{itemize}
\textit{Continuity} is defined as  
\begin{align}
  \label{eq:continuity}
  \lim_{x\rightarrow a} f(x) = f(a)
\end{align}
where $x$ and $a$ are independent points of each other. If \ref{eq:continuity} is satisfied for every point in the set $a$ then the function is \textit{continuous}, otherwise it is \textit{discontinuous}. If the set is distinct and unconnected then the function is \textit{discrete}, typically integers or whole numbers. In global optimization knowing the continuity can determine if a specific type of algorithm is needed. For discrete functions mixed integer algorithms are used
as they pick points in integer domain. In the case of discontinuous functions, the objective function, space or constraints are modified to make the function either continuous or seem continuous to the solver. Examples of these modifications is
\begin{itemize}
    \item generate a bad result value for gap or asymptotic regions,
    \item constrain the space to contain the continuous regions of the function,
    \item add constraints to avoid gap or asymptotic regions.
\end{itemize}

\textit{Smoothness} is defined as the order of continuous derivatives the function can achieve. The higher the order of the continuous derivative the smoother the function is. For global optimization the smoother the function the easier it is to converge on a minimum. However, in the case of non-smooth functions for example objective functions based on probabilities, the data becomes noise. In this case the objective function or the algorithm need to smooth out the data by either averaging
the results with in the objective function or the algorithm re-evaluates the point several times to obtain the best or averaged value. <add example of algorithm that does this>

\textit{Convexity} is defined as the curvature of the function. A function is \textit{convex} if for any $x_1,x_2 \epsilon X$ that follows
\begin{equation}
    \label{eq:convexity}
    f(\lambda x_1 + (1-\lambda)x_2) \leq \lambda f(x_1)+(1-\lambda)f(x_2),\ 0 \leq \lambda \leq 1.
\end{equation}

If the condition \ref{eq:convexity} does the hold the function is \textit{non-convex}. The convexity of the function also indicates the difficulty to optimize the function, shown in Figure \ref{fig:convexity}, the local minimum of a convex function is also the global minimum (or concave for local maximum), thus a local optimization solver can solves these types of functions 

\begin{figure}[!ht]
  
  \begin{subfigure}[t]{0.5\textwidth}
    \begin{tikzpicture}
      \begin{axis}[xlabel = {$x$},ylabel = {$f(x)$}]
		  \addplot [domain=-1:1,samples=10,color=red]{x^2};
        \end{axis}
      \end{tikzpicture}
      \caption{}
    \end{subfigure}
    \begin{subfigure}[t]{0.5\textwidth}
      \begin{tikzpicture}
		\begin{axis}[xlabel = {$x$},ylabel = {$f(x)$},]
          \addplot [domain=-1:1,samples=10,color=blue]{x^2*sin(x)};
        \end{axis}
      \end{tikzpicture}
      \caption{}
    \end{subfigure}
  \caption{Comparison between a convex and non-convex function.}
  \label{fig:convexity}
\end{figure}


When the function is non-convex the local minimum is not guarantee to be the global minimum. In this case a global optimization solver is needed to search the whole domain to obtain the global minimum. 

\textit{Differentiable} is defined if a functions derivative exists. If a functions derivative does exist, then it can be used to aid global optimization solver to converge to solution faster. The solver uses the derivative to determine the slope at a given point, if 
\begin{equation}
    f'(x) = 0,
\end{equation}
then a local minimum or maximum is obtained.

In an objective function this is implemented by returning an extra output or implementing another function that returns the derivative to the solver. 

\textit{computational time} is the time it takes to evaluate the objective function at a given point. In global optimization this is a factor in optimization time and if concurrency is needed. To speed up the optimization time the objective function can be come parallel if possible or the use of a parallel algorithm, for example parallel particle swarm optimization. 

Objective functions are the core of global optimizations that can aid in the optimization process and make it challenging. As discussed in this section certain characteristics are also used to determine the algorithm needed to be used on the function. In the situation when then objective functions characteristics are unknown or cannot provide extra information like the derivative, a \textit{black box global
optimization} solver is used. Black box refers to a unknown objective function that in takes in input information then returns a value.  


\section{Constraints}
Constraints are conditions placed by the user or the algorithm that must satisfy a given condition. For user defined constraints they can be implicit or explicit constraints that need to satisfy a condition. \textit{Explicit constraints} satisfy an equality or inequality condition, typically:
\begin{align}
    \label{eq:constraints}
    g_i(x) = 0, 
    h_j(x) \leq 0 
\end{align}
where $i$th and $j$th condition value. In the event the constraints are not satisfied the algorithm will apply a penalty to evaluated position. Penalties vary based on algorithms they can be:
\begin{itemize}
    \item function set by the user,
    \item increase (or decrease) the returned value by a factor, 
    \item increase search basin for local search region.
\end{itemize}

\textit{implicit constraints} are constraints set within the objective function. This is occurs when algorithms does not provide the option to set explicit constraint functions. In this case the user will then implement a condition in the objective function that will return a penalized value. An algorithm that incorporates this is the \textit{augmented Lagrangian method} developed by \cite{}, where explicit constraints are combined with the objective function to create a
sub-problem that is optimized by a local solver. When constraints are not met the sub-problem will penalize the value and the process will be repeated until a solution is obtained. To obtain global minimum global this algorithm, locally optimizes several regions of the search space to obtain the global minimum.

For algorithm conditions, these are set with in the algorithm to restrict the search space or internal algorithm parameters. An example of algorithm constraints is used in the $\alpha$ Branch and Bound algorithm developed by \cite{}, where internal constraints were placed on the lower and upper bounds of the objective function to segment sections that made the function convex. This is a form of \textit{convex relaxation}, where certain manipulations to non-convex functions
changes the function to be convex. In this situation the bounds were changed until the function is convex to then converge on a local minimum in that region. The algorithm then repeats for other regions and compares to give a final global minimum solution. 

Other algorithm constraints are seen as termination conditions, for example
\begin{itemize}
    \item number of evaluations,
    \item time limit,
    \item function tolerance,
    \item position tolerance, and
    \item no change in current best solution.
\end{itemize}

Any of the above conditions set in an algorithm needs to be satisfied before termination of the global optimization process. 

\section{Algorithms}
In the past recent years global optimizations algorithms are broken into two main types, stochastic and deterministic. 
\subsection{Deterministic}
Deterministic algorithms guarantee finding the global minima by searching the whole
domain with tight convergence properties \cite{Pinter2002}. In 1969 the branch and bound algorithm \cite{Liberti2000} was one of the most well known algorithms for solving complex models like the travelling salesman problem \cite{Liberti2000}. However, this algorithm was restricted to specific types of problems like concave minimization and when computing worst case, its time complexity was exponential.
\subsection{Stochastic}
Stochastic algorithms were developed in the seventies to use adaptive random methods to obtain a feasibly close global minimum solution in a reasonable amount of time to the user. These algorithms, unlike deterministic algorithms, cannot guarantee finding a global minimum. Because of this property of stochastic algorithms, further research has become popular in obtaining a better global minimum for different classifications of problems \cite{Aguiar,Pinter2002}. Various sub-categories of stochastic algorithms have appeared, like probabilistic approaches, Monte Carlo approaches, evolutionary algorithms \cite{Aguiar} and metaheuristics methods
\cite{Can2015}. Each sub-category target different problems
and shows various improvement on different type of functions. Some common well known algorithms include: particle swarm optimization (evolutionary algorithm), differential evolution (metaheuristic method), genetic algorithm (evolutionary algorithm), cross-entropy method (Monte-Carlo algorithm), and simulated annealing (probabilistic algorithm) \cite{Aguiar}.  

\subsubsection{Global Search}
\textit{Global Search} (GS) is hybrid heuristic algorithm that generates population points using the scatter search algorithm \cite{Glover1998} and a local solver to optimize around the points. GS starts by locally optimizing around the initial point, $x0$, the user provides to the algorithm. If the local optimization converges various parameters are recorded
\begin{itemize}
    \item initial point,
    \item convergent point,
    \item final object function value, and
    \item score value. 
\end{itemize}

The \textit{score value} is determined by taking the sum of the objection function value and any constraint violations. If point is feasible then the score value is equal to the objective function value. Otherwise every constraint not satisfied adds on additional constraint violation value, typically a thousand. This is a form of a penalty function to avoid exploration around points that do not satisfy the constraints. 

The algorithm will then generate trials points using the scatter search algorithm and evaluate each point for their score value. The point with the best score value is then optimized with the local solver. The same information is stored on this trial point as the initial point. 

The algorithm then initialize the basins of attraction value and the radius. \textit{Basins of attraction} are the heuristic assumption to be spherical. Thus two spheres are centred around the convergent points of the initial and best trial points with the radii being the distance between the start points to the convergent points of the local optimization. These estimated basins can overlap. 

A local solver threshold is initialized to be less than the two convergent objective function values, if those points scare values are infeasible then the value is equal to score value of the first trial point. 

Two counters are initialized to zero that are associated with number of consecutive trial points that lie with in a basin of attraction (counter per basin) and when a score value is greater than the local solver threshold. 

The algorithm then continues to evaluate each trial point using the local optimizer if the following conditions hold


\begin{itemize}
\item condition 1
    \begin{equation}
        \label{eq:condition1}
        | x_i - b_j | > dr_j
    \end{equation}
        where $x_i$ is the $i$th trial point and $b_j$ is the $j$th basin of attraction centre; $d$ is the distance threshold factor, default value $0.75$, and $r_j$ is the radius of $j$th basin of attraction. 
    \item Condition 2 \[ score(x_i) < l\] where l is local solver threshold.
    \item Condition 3 (optional) $x_i$ satisfies bound and inequality constraints.
\end{itemize}
If all conditions are met then the local solver runs on the trial point, $x_i$. If the local solver converges then the global optimum solution is updated if one of the following conditions is satisfied
\begin{align}
    \label{eq:updateglobal}
    |xc_k - xc_i | > T_x \max ( 1,|xc_i| ) \\
    or \\
    |fc_k - fc_i | > T_f \max ( 1,|fc_i| )  
\end{align}
where $xc_k$ and $xc_i$ is every $kth$ convergent point and the convergent point for the $i$th trial point; $fc_k$ and $fc_i$ is every $k$th objective function value and objective function for the convergent point for the $i$th trial point; $T_x$ and $T_f$ are the $x$ tolerance and function tolerance, default $1\ 10^-8$. 

Likewise the basin radius and local solver threshold is updated if the local solver converges. The updates are as follows
\begin{itemize}
    \item threshold is set to the score value at the trial point, and
    \item basin radius is set to the $xc_i$ to $x_i$ distance and maximum existing radius (if any).
\end{itemize}

If the local does not run on the trial point due to the conditions not being satisfied the two counters are incremented. The first counter is the basin counter, it increments for each basin $b_j$ that $x_i$ is in. The second counter is the threshold counter, it increments if $score(x_i) \geq l$ otherwise reset to 0. At the beginning of algorithm both counters are set to zero. 

When each basin counter is equal to maximum counter value then basin radius is multiplied by one minus the basin radius factor and reset the basin counter to zero. If the threshold counter is equal to the maximum counter value then increase the local solver threshold to
\begin{equation}
    l = l + p_f(1+|l|)
\end{equation}
where $p_f$ is a penalty threshold factor, then reset the counter to zero.

\subsubsection{Particle Swarm Optimization}
Particle Swarm Optimization (PSO) is a black box algorithm developed in 1995 by Kennedy and Eberhart \cite{Kennedy}. This algorithm is a nature-inspired algorithm from a simplified social swarm model, where the algorithm mimics the social behaviour of flocking birds. Each particle is assigned a position, once evaluated a global best is assigned to a particle position then the other particles obtain a stochastic velocity to swarm to wards the global best particle. The stochastic velocity is
based on experience of the specific particle and the knowledge of the swarm
\begin{equation}
    \label{eq:velocity}
    v^{k+1}_{i,j} = v^{k}_{i,j} + c_1r_1(x^{k}_b - x^{k}_{i,j}) + c_2r_2(x^{k}_g - x^{k}_{i,j})
\end{equation}
where $x^k_{i,j}$ and $v^k_{i,j}$ are the $j$th component of the $i$th particle's position and velocity vector in the $k$th iteration; $r_1$ and $r_2$ are two random numbers, $x_b$ and $x_g$ are the best position the particle experienced and the global best in the swarm; $c_1$ and $c_2$ are two parameters that represents the particle's confidence in itself and the swarm. The position of the particle is then updated 
\begin{equation}
    \label{eq:position}
    x^{k+1}_{i,j} = x^k_{i,j} + v^{k+1}_{i,j}
\end{equation}
using the velocity obtained by Equation \ref{eq:velocity}. To avoid premature convergence on local minima or over exploration of the particles Shi and Eberhart \cite{} introduced a new term known as the \textit{inertia weight}, w. The inertia weight balances out the premature convergence and over exploration problems by influencing the previous velocity term
\begin{equation}
    \label{eq:velocity2}
    v^{k+1}_{i,j} = wv^{k}_{i,j} + c_1r_1(x^{k}_{local} - x^{k}_{i,j}) + c_2r_2(x^{k}_{global} - x^{k}_{i,j}).
\end{equation}

This added term showed overall performance increase in the standard PSO algorithm. Then later Clerc \cite{} used a constriction factor to insure convergence in the particle swarm. This altered the Equation \ref{eq:velocity} to 
\begin{align}
    \label{eq:velocity3}
    v^{k+1}_{i,j} = \chi \big[ v^{k}_{i,j} + c_1r_1(x^{k}_b - x^{k}_{i,j}) + c_2r_2(x^{k}_g - x^{k}_{i,j}) \big] \\
    \chi = \frac{2}/{\big| 2- \phi - \sqrt{\phi^2 - 4\phi}\big|}\ where\ \phi = c_1 + c_2,\ \phi>4,
\end{align}
this equation also prevents particle divergence. The schematic movement of the particle shown in Figure \ref{fig:particle movement} follows the Equation \ref{eq:velocity3}.

\begin{figure}
    \centering
    \includestandalone[width=\textwidth]{chapters/chapter_2_Background/particle_movement}
    \caption{Schematic of the particles movement using Equation \ref{eq:velocity2}}
    \label{fig:particle movement}
\end{figure}

When compared to the inertia weight Equation \ref{eq:velocity}, Shi and Eberhart found they were equivalent in performance \cite{}. 
Thus either velocities align are used in the standard PSO algorithm: 
\begin{algorithm}[H]
  \begin{algorithmic}[1]

    \For{each particle i}
        \State \textbf{initialization} $x_i$, $v_i$, $xbest_i$ \Comment{random value for $x_i$ and $v_i$}
        $xbest_i \gets xbest_i$
        \State \textbf{Evaluate} $f(x_i)$  \Comment{evaluate the objective function at $x_i$}
        \State \textbf{Update} $xbest_i$ \Comment{update if $f(x_i) < f(xbest_i)$}
    \EndFor
    \While{not termination condition}
        \For{each particle i}
            \State \textbf{update} xglobal \Comment{update if $f(xglobal) < f(xbest_i)$}
            \State \textbf{calculate} $v_i$ \Comment{Using one of the PSO velocity equations}
            \State $x_i = x_i + v_i$
            \State \textbf{Evaluate} $f(x_i)$
            \State \textbf{update} $xbest_i$
        \EndFor
    \EndWhile
  \end{algorithmic}
\caption{Particle Swarm Optimization}
\label{algorithmPSO}
\end{algorithm}

In the past recent years multiple variants of the PSO algorithm have been developed, collision-free PSO developed by Krink et al. \cite{},. One variant of PSO known Global Convergence PSO, used to solve the crystal structure prediction problem discussed in Chapter \ref{applications}. Van den Bergh and Engelbrecht \cite{} developed \textit{Guaranteed Convergence PSO} (GCPSO) that particles preform a random search around the global best particle within a dynamic adapted radius.
This encourages local convergence and address stagnation by randomly searching around the global best particle in an adaptive radius at each iteration. The GCPSO uses the align \ref{eq:velocity2} and \ref{eq:velocity3} to determine the particles velocity, $v^k_{i,j}$, and to update the inertia weight factor, $w$, over each iteration. Then updating the global best particles velocity by
\begin{align}
    \label{eq:globalvelocity}
    v^{k+1}_{i_g} = -x^k_{i_g} + x^k_{i_g} + w^k v^k_{i_g} + \rho^k(1-2r^k_3) \\
\end{align}
where $i_g$ is the index of the particle that is most recently update the global best value. $r_3$ is a random number between $(0,1)$. The search radius, $\rho^k$, is calculated by
\begin{eqnarray}
    \rho^{k+1} & = &
    \begin{cases}
        2\rho^k, &\text{if} \check{s}^{k+1} > s_c,\\
        \frac{1}{2}\rho^k, &\text{if} \check{a}^{k+1}>a_c\\
        \rho^k, & \mbox{otherwise}
    \end{cases}
    \label{eq:rho}
\end{eqnarray}
where $s_c$ is the success threshold and $a_c$ is the failure threshold. A success is indicated when Equation \ref{eq:globalvelocity} results in an improved global best value, otherwise it is a failure. Each time a consecutive success occurs $s_c$ increase by one otherwise it is set back to zero if a failure occurs and vice versa for $a_c$. 

    

